{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81eb9700",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Getting the dataset \n",
    "from tensorflow.keras.datasets import mnist\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1800f55",
   "metadata": {},
   "outputs": [],
   "source": [
    " #Loads data into training and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17fdd9f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (60000,)\n",
      "(10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape, y_train.shape)\n",
    "print(x_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7abafe5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57f0dd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape((x_train.shape[0], 28, 28, 1)).astype('float32') / 255.0\n",
    "x_test = x_test.reshape((x_test.shape[0], 28, 28, 1)).astype('float32') / 255.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad55f167",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28, 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5165050e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "091f6d7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.]], shape=(60000, 10))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95cebefc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28, 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65301569",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4731fc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HandwrittenNumberClassification:\n",
    "    def __init__(self, batch_size = 32, channel_size = 1, kernal_size = 4, filters = 8, stride = 4, pool_size = 4,lr=0.01):\n",
    "        self.lr = lr\n",
    "        self.B = batch_size\n",
    "        self.C_in = channel_size\n",
    "        self.KH = kernal_size ## Kernal height\n",
    "        self.KW = kernal_size ## Each kernal width\n",
    "        self.F = filters\n",
    "        self.stride = stride\n",
    "        self.pool_size = pool_size\n",
    "        self.kernel = np.random.randn(self.F, self.C_in, self.KH, self.KW)\n",
    "        self.input = None\n",
    "\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        \"\"\"\n",
    "        x: (B, num_classes)\n",
    "        Returns: probabilities for each class per sample\n",
    "        \"\"\"\n",
    "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))  # stability trick\n",
    "        probs = exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "        return probs\n",
    "    \n",
    "    def compute_loss(self, probs, y_true):\n",
    "        \"\"\"\n",
    "        probs: (B, num_classes)\n",
    "        y_true: (B,)\n",
    "        \"\"\"\n",
    "        B = y_true.shape[0]\n",
    "        y_encoded = np.eye(self.no_of_outputs)[y_true]\n",
    "        log_likelihood = -np.log(probs[range(B), y_true] + 1e-9)\n",
    "        loss = np.sum(log_likelihood) / B\n",
    "        return loss\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    def generate_feature_map(self, X=None):\n",
    "\n",
    "        ''' \n",
    "        It will take two things:\n",
    "        1. Kernal = (filters,Channel_input, kernal_Height, kernal_width)\n",
    "        2. Input = (Batch , sequence_size_width, sequence_size_height, Channel_input)\n",
    "        \n",
    "        It will give output : feature_maps: list of shape(batch, Filters, L_output_height, l_output_width)\n",
    "        L_output_height = Sequence_length - kernal_Height + 1\n",
    "        L_output_width = Sequence_length - kernal_width + 1\n",
    "\n",
    "        '''\n",
    "        if X is None:\n",
    "            X = self.input\n",
    "\n",
    "        B, H, W, C_in = X.shape\n",
    "        F,_,KH,KW = self.kernel.shape\n",
    "        L_out_h = X.shape[1] - KH + 1\n",
    "        L_out_w = X.shape[2] - KW + 1\n",
    "\n",
    "        ## Initialize feature map\n",
    "        self.feature_map = np.zeros((B,F,L_out_h,L_out_w))\n",
    "\n",
    "        ## Start the sliding window technique\n",
    "        for b in range(B):\n",
    "            for f in range(F):\n",
    "                feature = np.zeros((L_out_h, L_out_w))\n",
    "                for c in range(C_in):\n",
    "                    x = X[b, :, :, c]\n",
    "                    k = self.kernel[f, c]\n",
    "                    for i in range(L_out_h):\n",
    "                        for j in range(L_out_w):\n",
    "                            window = x[i:i+KH, j:j+KW]\n",
    "                            feature[i, j] += np.sum(window * k)\n",
    "                self.feature_map[b, f] = feature\n",
    "        return self.feature_map\n",
    "    \n",
    "    def relu(self,x):\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def generate_max_pool(self,x):\n",
    "        '''\n",
    "        x: shape (B,F,L_out_h, L_out_w)\n",
    "        \n",
    "        This function does 2 important work:\n",
    "        1. Generate a pooled map using pool_size and stride.\n",
    "        2. Generate Binary map with address of the max value that we will select, it will help us to \n",
    "        understand and locate when distributing loss in backpropogation\n",
    "\n",
    "\n",
    "        Returns : Pooled output of shape (B,F,L_pooled)\n",
    "        '''\n",
    "\n",
    "        B,F,LH,LW = x.shape\n",
    "        L_pool_h = (LH-self.pool_size)//self.stride + 1\n",
    "        L_pool_w = (LW-self.pool_size)//self.stride + 1\n",
    "\n",
    "        pooled = np.zeros((B,F,L_pool_h,L_pool_w))\n",
    "        self.binary_max_pool = np.zeros((B,F,LH,LW)) ## Since we need binary of the shape as feature map\n",
    "\n",
    "        for b in range(B):\n",
    "            for f in range(F):\n",
    "                for i in range(L_pool_h):\n",
    "                    for j in range(L_pool_w):\n",
    "                        start_h = i * self.stride\n",
    "                        end_h = start_h + self.pool_size\n",
    "                        start_w = j * self.stride\n",
    "                        end_w = start_w + self.pool_size\n",
    "\n",
    "                        window = x[b, f, start_h:end_h, start_w:end_w]\n",
    "                        max_val = np.max(window)\n",
    "                        pooled[b, f, i, j] = max_val\n",
    "\n",
    "                        # Binary mask for backprop (optional for now)\n",
    "                        max_indices = np.where(window == max_val)\n",
    "                        self.binary_max_pool[b, f, start_h:end_h, start_w:end_w][max_indices] = 1\n",
    "        return pooled\n",
    "    \n",
    "    def generate_flattened_array(self, pooled_array):\n",
    "        '''\n",
    "        pooled array : [B,F,L_pooled_H, L_pooled_W]\n",
    "        '''\n",
    "        B= pooled_array.shape[0]\n",
    "        return pooled_array.reshape(B,-1)\n",
    "    \n",
    "    def unpool(self,pooled_grad, binary_mask):\n",
    "        '''\n",
    "        pooled_grad : (B,F,L_Pooled_H, L_pooled_w)\n",
    "        Binary_mask : (B,F,LH,LW)\n",
    "        '''\n",
    "\n",
    "        B,F,LH,LW = binary_mask.shape\n",
    "        _,_,L_pool_H,L_pool_W = pooled_grad.shape\n",
    "\n",
    "        unpooled = np.zeros_like(binary_mask, dtype=pooled_grad.dtype)\n",
    "\n",
    "        for b in range(B):\n",
    "            for f in range(F):\n",
    "                for i in range(L_pool_H):\n",
    "                    for j in range(L_pool_W):\n",
    "                        start_h = i * self.stride\n",
    "                        end_h = start_h + self.pool_size\n",
    "                        start_w = j * self.stride\n",
    "                        end_w = start_w + self.pool_size\n",
    "\n",
    "                        ## Gradient for this reagion\n",
    "                        grad = pooled_grad[b,f,i,j]\n",
    "\n",
    "                        ## Distributing gradient only to maximum value of that pool\n",
    "                        mask_window = binary_mask[b,f,start_h:end_h, start_w:end_w]\n",
    "                        unpooled[b,f,start_h:end_h, start_w:end_w] += mask_window * grad\n",
    "\n",
    "        return unpooled\n",
    "        \n",
    "    def generate_kernal_loss(self,relu_grad):\n",
    "        '''\n",
    "        relu_grad = Shape (B,F,LH,LW)\n",
    "        Filter = shape(F,C_in,KH,KW)\n",
    "        input = shape(B,H,W,C_in)\n",
    "\n",
    "        return dl_dk = shape(F,C_in,KH,KW)\n",
    "        '''\n",
    "\n",
    "        B,F,LH,LW = relu_grad.shape\n",
    "        _,C_in,KH,KW = self.kernel.shape\n",
    "        _,H,W,_ = self.input.shape \n",
    "        L_out_H = LH - KH + 1\n",
    "        L_out_W = LW - KW + 1\n",
    "\n",
    "        dl_dk = np.zeros_like(self.kernel)\n",
    "\n",
    "\n",
    "        for f in range(F):\n",
    "            for c in range(C_in):\n",
    "                for b in range(B):\n",
    "                    x = self.input[b, :, :, c] ## Shape (H,W)\n",
    "                    grad_out = relu_grad[b,f] ## Shape (LH,LW) : it means take windows from x, each window has it's corresponding loss\n",
    "                    for i in range(L_out_H):\n",
    "                        for j in range(L_out_W):\n",
    "                            ## Let's make a window so that each element from that window will get multiplied to a kernal value and then we can get the loss\n",
    "                            window = x[i:i+KH,j:j+KW]\n",
    "\n",
    "                            ## Multiply input gradient with output value and then add to loss of each kernal value\n",
    "                            dl_dk[f,c] += window * grad_out[i,j]\n",
    "\n",
    "        ## Taking average of batch to remove the factor of big or small batch\n",
    "        dl_dk /= self.B\n",
    "\n",
    "        return dl_dk\n",
    "\n",
    "\n",
    "    def feature_extraction(self):\n",
    "        feature_map = self.generate_feature_map()\n",
    "        self.relu_feature_map = self.relu(feature_map)\n",
    "        self.max_pool = self.generate_max_pool(self.relu_feature_map)\n",
    "\n",
    "        return self.max_pool\n",
    "    \n",
    "    def forward_decision_making(self, pooled_array):\n",
    "        '''\n",
    "        pooled array : [B,F,L_pooled_H, L_pooled_W]\n",
    "        '''\n",
    "        B,F,L_pooled_H,L_pooled_W = pooled_array.shape\n",
    "        #Now Flapttened array\n",
    "\n",
    "        self.flattened_array = self.generate_flattened_array(pooled_array)\n",
    "\n",
    "        ## Now pass it with dense layer\n",
    "        N_in = F * L_pooled_H * L_pooled_W\n",
    "        self.no_of_outputs = 10\n",
    "        self.W_dense = np.random.randn(N_in, self.no_of_outputs)\n",
    "        self.B_dense = np.random.randn(self.no_of_outputs)\n",
    "\n",
    "        output = self.flattened_array @ self.W_dense + self.B_dense\n",
    "\n",
    "        return output # [B,no_of_outputs]\n",
    "        \n",
    "\n",
    "    def forward(self):\n",
    "        max_pooled_arr = self.feature_extraction()\n",
    "        final_decision = self.forward_decision_making(max_pooled_arr)\n",
    "        self.probabilities = self.softmax(final_decision)\n",
    "        return self.probabilities\n",
    "    \n",
    "    def backprop_decision_making(self,y_train):\n",
    "\n",
    "        y_one_hot = np.eye(self.no_of_outputs)[y_train]  # Shape: (B, 10)\n",
    "\n",
    "        # Compute gradient of loss w.r.t. logits (softmax cross-entropy derivative)\n",
    "        dl_dz = y_one_hot - self.probabilities  # Shape (B, 10)\n",
    "        dl_dw = self.flattened_array.T @ dl_dz / self.flattened_array.shape[0] ## flattened array's first item in it's shape will give the B value so shape is [F*L_pool_h*L_pool_w,no_of_outputs]\n",
    "        dl_db = np.mean(dl_dz,axis=0)\n",
    "\n",
    "        ## Loss with flattened array to just propogate further\n",
    "        dl_input = dl_dz @ self.W_dense.T # Shape [B,F*L_pool_h*L_pool_w]\n",
    "\n",
    "        self.W_dense -= self.lr * dl_dw\n",
    "        self.B_dense -= self.lr * dl_db\n",
    "\n",
    "        return dl_input\n",
    "    \n",
    "    def backprop_convolution(self,y_train):\n",
    "        dl_input = self.backprop_decision_making(y_train)\n",
    "\n",
    "        ## We need to unflatten this before going to unpooling and passing loss to kernel\n",
    "        dl_unflatten_input = dl_input.reshape(self.B, self.F, self.max_pool.shape[2], self.max_pool.shape[3]) ## Shape [B,F,L_Pooled_H, L_pooled_w]\n",
    "\n",
    "        ## Unpooling of loss\n",
    "        dl_unpooled_grad = self.unpool(dl_unflatten_input,self.binary_max_pool) ## Shape (B,F,LH,LW)\n",
    "\n",
    "        ## Calculating loss with relu layer\n",
    "        dl_drelu_grad = dl_unpooled_grad * (self.relu_feature_map > 0) ## Shape (B,F,LH,LW)\n",
    "\n",
    "        ## Passing loss to kernal through feature map\n",
    "        dk = self.generate_kernal_loss(dl_drelu_grad)\n",
    "\n",
    "        ## Update kernal values\n",
    "        self.kernel -= self.lr * dk\n",
    "\n",
    "    def train_model(self, X, y, epochs=5):\n",
    "        \"\"\"\n",
    "        X: shape (num_samples, 28, 28, 1)\n",
    "        y: shape (num_samples, 10) or (num_samples,) -> one-hot or integer labels\n",
    "        \"\"\"\n",
    "        num_samples = X.shape[0]\n",
    "        num_batches = num_samples // self.B\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            epoch_loss = 0\n",
    "\n",
    "            # Shuffle data at the start of each epoch\n",
    "            indices = np.arange(num_samples)\n",
    "            np.random.shuffle(indices)\n",
    "            X = X[indices]\n",
    "            y = y[indices]\n",
    "\n",
    "            for batch_idx in range(num_batches):\n",
    "                # Create mini-batch\n",
    "                start = batch_idx * self.B\n",
    "                end = start + self.B\n",
    "                X_batch = X[start:end]\n",
    "                y_batch = y[start:end]\n",
    "\n",
    "                # ✅ If labels are one-hot, convert to integer labels\n",
    "                if y_batch.ndim > 1:\n",
    "                    y_batch_indices = np.argmax(y_batch, axis=1)\n",
    "                else:\n",
    "                    y_batch_indices = y_batch\n",
    "\n",
    "                # Save input for convolution\n",
    "                self.input = X_batch\n",
    "\n",
    "                # Forward pass\n",
    "                probs = self.forward()  # -> (B, 10)\n",
    "            \n",
    "                # Compute loss\n",
    "                loss = self.compute_loss(probs, y_batch_indices)\n",
    "                epoch_loss += loss\n",
    "\n",
    "                # Backward pass\n",
    "                self.backprop_convolution(y_batch_indices)\n",
    "\n",
    "                if batch_idx % 10 == 0:\n",
    "                    print(f\"Epoch {epoch+1}, Batch {batch_idx}/{num_batches}, Loss: {loss:.4f}\")\n",
    "\n",
    "            avg_epoch_loss = epoch_loss / num_batches\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}]  Loss: {avg_epoch_loss:.4f}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        X: shape (num_samples, H, W, C)\n",
    "        Returns: predicted class indices -> (num_samples,)\n",
    "        \"\"\"\n",
    "        num_samples = X.shape[0]\n",
    "        all_preds = []\n",
    "\n",
    "        # Process in batches for efficiency\n",
    "        num_batches = int(np.ceil(num_samples / self.B))\n",
    "        for batch_idx in range(num_batches):\n",
    "            start = batch_idx * self.B\n",
    "            end = min(start + self.B, num_samples)\n",
    "\n",
    "            X_batch = X[start:end]\n",
    "            self.input = X_batch\n",
    "\n",
    "            probs = self.forward()  # -> (B, 10)\n",
    "            preds = np.argmax(probs, axis=1)\n",
    "            all_preds.extend(preds)\n",
    "\n",
    "        return np.array(all_preds)\n",
    "\n",
    "\n",
    "    def evaluate(self, X_test, y_test):\n",
    "        \"\"\"\n",
    "        X_test: shape (num_samples, H, W, C)\n",
    "        y_test: shape (num_samples, 10) or (num_samples,) -> one-hot or integer labels\n",
    "        Returns: accuracy (float)\n",
    "        \"\"\"\n",
    "        # ✅ Convert one-hot labels to integers if necessary\n",
    "        if y_test.ndim > 1:\n",
    "            y_true = np.argmax(y_test, axis=1)\n",
    "        else:\n",
    "            y_true = y_test\n",
    "\n",
    "        # Get predictions\n",
    "        y_pred = self.predict(X_test)\n",
    "\n",
    "        # Compute accuracy\n",
    "        accuracy = np.mean(y_pred == y_true)\n",
    "        print(f\"Evaluation Accuracy: {accuracy * 100:.2f}%\")\n",
    "        return accuracy\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172415ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 0/3, Loss: 17.4368\n",
      "Epoch [1/10]  Loss: 16.5764\n",
      "Epoch 2, Batch 0/3, Loss: 18.4559\n",
      "Epoch [2/10]  Loss: 17.2652\n",
      "Epoch 3, Batch 0/3, Loss: 17.9892\n",
      "Epoch [3/10]  Loss: 18.2082\n",
      "Epoch 4, Batch 0/3, Loss: 15.1071\n",
      "Epoch [4/10]  Loss: 15.1830\n",
      "Epoch 5, Batch 0/3, Loss: 18.1809\n",
      "Epoch [5/10]  Loss: 18.1201\n",
      "Epoch 6, Batch 0/3, Loss: 16.0751\n",
      "Epoch [6/10]  Loss: 18.0196\n",
      "Epoch 7, Batch 0/3, Loss: 14.2472\n",
      "Epoch [7/10]  Loss: 17.2853\n",
      "Epoch 8, Batch 0/3, Loss: 13.9096\n",
      "Epoch [8/10]  Loss: 16.9389\n",
      "Epoch 9, Batch 0/3, Loss: 17.3788\n",
      "Epoch [9/10]  Loss: 16.8857\n",
      "Epoch 10, Batch 0/3, Loss: 19.5680\n",
      "Epoch [10/10]  Loss: 18.5139\n"
     ]
    }
   ],
   "source": [
    "model = HandwrittenNumberClassification()\n",
    "## Since it is very slow I used smaller dataset.\n",
    "model.train_model(X=x_train[:100],y=y_train[:100],epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937afae0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
